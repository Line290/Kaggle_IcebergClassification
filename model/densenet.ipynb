{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1711.09268.pdf\n",
      "20180108_submission.csv\n",
      "hoffman17a.pdf\n",
      "sample_submission.csv\n",
      "submission0.13280066598320180104.csv\n",
      "submission0.16253202607520180104.csv\n",
      "submission0.19644353542120180104.csv\n",
      "submission0.19657285310120180104.csv\n",
      "submission0.20325070297420180104.csv\n",
      "submission0.22419952220220180104.csv\n",
      "submission0.2447353166520180104.csv\n",
      "test.json\n",
      "test_submission_0.1328_180104.csv\n",
      "test_submission_0.15153_180104.csv\n",
      "test_submission_0.17767_2.csv\n",
      "test_submission_1211.csv\n",
      "test_submission_1212[0.15153096562327303, 0.95248868805250975].csv\n",
      "test_submission_1212[0.17714124068415543, 0.93212670060843905].csv\n",
      "test_submission_1212[0.19647113115027331, 0.91938405797101452].csv\n",
      "test_submission_1212[0.19795040937437527, 0.90851449275362317].csv\n",
      "test_submission_1212[0.21496757851466874, 0.92081448152593892].csv\n",
      "test_submission_1212[0.22653280932377473, 0.89879154114564741].csv\n",
      "test_submission.csv\n",
      "train.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "INFO:__main__:USE CUDA=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 2.7.12 (default, Nov 20 2017, 18:23:56) \n",
      "[GCC 5.4.0 20160609]\n",
      "__pyTorch VERSION: 0.3.0.post4\n",
      "2.7.12 (default, Nov 20 2017, 18:23:56) \n",
      "[GCC 5.4.0 20160609]\n",
      "0.0\n",
      "svmem(total=67437826048, available=62890123264, percent=6.7, used=2871574528, free=52861157376, active=3875618816, inactive=9767325696, buffers=2959917056, cached=8745177088, shared=989982720)\n",
      "memory GB: 0.162033081055\n",
      "(1604, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 2, 75, 75)\n",
      "<type 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1604, 2, 75, 75])\n",
      "(1604, 1)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "(1604, 1)\n",
      "<type 'numpy.ndarray'>\n",
      "Offest:1427\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f5664a89e90>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f5664a89b50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n",
      "INFO:__main__:<torch.optim.adam.Adam object at 0x7f5664a89ad0>\n",
      "INFO:__main__:BCELoss(\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  + Number of params: 19921\n",
      "DenseNet(\n",
      "  (conv1): Conv2d (2, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (dense1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv1): Conv2d (16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d (32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv1): Conv2d (24, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d (32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (trans1): Transition(\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (dense2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv1): Conv2d (16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d (32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv1): Conv2d (24, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d (32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (trans2): Transition(\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (conv1): Conv2d (32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (dense3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv1): Conv2d (16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d (32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv1): Conv2d (24, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d (32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (fc): Linear(in_features=128, out_features=1)\n",
      ")\n",
      "Epoch 1/5\n",
      "*****:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-1-1ed6f5a06bfe>\", line 233, in __getitem__\n    return self.full_ds[i+self.offset]\n  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataset.py\", line 39, in __getitem__\n    return self.data_tensor[index], self.target_tensor[index]\nRuntimeError: CUDA error (3): initialization error\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1ed6f5a06bfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mrunning_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-1-1ed6f5a06bfe>\", line 233, in __getitem__\n    return self.full_ds[i+self.offset]\n  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataset.py\", line 39, in __getitem__\n    return self.data_tensor[index], self.target_tensor[index]\nRuntimeError: CUDA error (3): initialization error\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "# Shlomo Kashani \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"./kaggle_lceberg_data\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "from sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "import pandas\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "handler=logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# !pip install psutil\n",
    "import psutil\n",
    "import os\n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "\n",
    "cpuStats()\n",
    "\n",
    "# use_cuda=False\n",
    "lgr.info(\"USE CUDA=\" + str (use_cuda))\n",
    "\n",
    "\n",
    "# #  Global params\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# fix seed\n",
    "seed=17*19\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "# #  View the Data\n",
    "# - Numerai provides a data set that is allready split into train, validation and test sets. \n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "# Data params\n",
    "TARGET_VAR= 'target'\n",
    "BASE_FOLDER = './kaggle_lceberg_data'\n",
    "\n",
    "\n",
    "# #  Train / Validation / Test Split\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# data = pd.read_json(BASE_FOLDER + '/train.json')\n",
    "data = pd.read_json(\"./kaggle_lceberg_data/train.json\")\n",
    "\n",
    "print (data.shape)\n",
    "# data['precision_4'] = data['inc_angle'].apply(lambda x: len(str(x))) <= 7\n",
    "# data = data[data['precision_4'] == True]\n",
    "# print (data.shape)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "def cross_image(im1, im2):\n",
    "   # get rid of the color channels by performing a grayscale transform\n",
    "   # the type cast into 'float' is to avoid overflows\n",
    "   im1_gray = np.sum(im1.astype('float'), axis=2)\n",
    "   im2_gray = np.sum(im2.astype('float'), axis=2)\n",
    "\n",
    "   # get rid of the averages, otherwise the results are not good\n",
    "   im1_gray -= np.mean(im1_gray)\n",
    "   im2_gray -= np.mean(im2_gray)\n",
    "\n",
    "   # calculate the correlation image; note the flipping of onw of the images\n",
    "   return scipy.signal.fftconvolve(im1_gray, im2_gray[::-1,::-1], mode='same')\n",
    "   \n",
    "# Suffle\n",
    "import random\n",
    "from datetime import datetime\n",
    "from scipy import signal\n",
    "random.seed(datetime.now())\n",
    "# np.random.seed(datetime.now())\n",
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data) # otherwise same validation set each time!\n",
    "data= data.reindex(np.random.permutation(data.index))\n",
    "\n",
    "data = shuffle(data) # otherwise same validation set each time!\n",
    "data= data.reindex(np.random.permutation(data.index))\n",
    "\n",
    "def Zpad(A, length):\n",
    "    arr = np.zeros(length)\n",
    "    arr[:len(A)] = A\n",
    "    return arr\n",
    "\n",
    "# data['band_1'] = data['band_1'].apply(lambda x: Zpad(x,6400))\n",
    "# data['band_2'] = data['band_1'].apply(lambda x: Zpad(x,6400))\n",
    "\n",
    "data['band_1'] = data['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "data['band_2'] = data['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "\n",
    "data['inc_angle'] = pd.to_numeric(data['inc_angle'], errors='coerce')\n",
    "\n",
    "import scipy\n",
    "band_1 = np.concatenate([im for im in data['band_1']]).reshape(-1, 75, 75)\n",
    "band_2 = np.concatenate([im for im in data['band_2']]).reshape(-1, 75, 75)\n",
    "# band_3=(band_1+band_2)/2\n",
    "# band_3=signal.fftconvolve(band_1, band_1, mode = 'same')\n",
    "\n",
    "full_img = np.stack([band_1, band_2], axis=1)\n",
    "\n",
    "# https://github.com/bermanmaxim/jaccardSegment/blob/master/compose.py\n",
    "\n",
    "# #  From Numpy to PyTorch GPU tensors\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def XnumpyToTensor(x_data_np):\n",
    "    x_data_np = np.array(x_data_np, dtype=np.float32)        \n",
    "    print(x_data_np.shape)\n",
    "    print(type(x_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")    \n",
    "        X_tensor = (torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")\n",
    "        X_tensor = (torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "        \n",
    "    print((X_tensor.shape)) # torch.Size([108405, 29])\n",
    "    return X_tensor\n",
    "\n",
    "\n",
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def YnumpyToTensor(y_data_np):    \n",
    "    y_data_np=y_data_np.reshape((y_data_np.shape[0],1)) # Must be reshaped for PyTorch!\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")            \n",
    "    #     Y = Variable(torch.from_numpy(y_data_np).type(torch.LongTensor).cuda())\n",
    "        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor).cuda()  # BCEloss requires Float        \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")        \n",
    "    #     Y = Variable(torch.squeeze (torch.from_numpy(y_data_np).type(torch.LongTensor)))  #         \n",
    "        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor)  # BCEloss requires Float        \n",
    "\n",
    "    print(type(Y_tensor)) # should be 'torch.cuda.FloatTensor'\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))    \n",
    "    return Y_tensor\n",
    "\n",
    "\n",
    "# #  Custom data loader\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "# transformations = transforms.Compose([transforms.Scale(32),transforms.ToTensor()])\n",
    "# preprocess = transforms.Compose([\n",
    "#   transforms.Scale(75),\n",
    "#   transforms.CenterCrop(224),\n",
    "#   transforms.ToTensor(),\n",
    "#   normalize\n",
    "# ])\n",
    "\n",
    "class FullTrainningDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, full_ds, offset, length):\n",
    "        self.full_ds = full_ds\n",
    "        self.offset = offset\n",
    "        self.length = length\n",
    "        assert len(full_ds)>=offset+length, Exception(\"Parent Dataset not long enough\")\n",
    "        super(FullTrainningDataset, self).__init__()\n",
    "        \n",
    "    def __len__(self):        \n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # label = torch.from_numpy(self.y_train[index])\n",
    "        return self.full_ds[i+self.offset]\n",
    "    \n",
    "validationRatio=0.11    \n",
    "\n",
    "def trainTestSplit(dataset, val_share=validationRatio):\n",
    "    val_offset = int(len(dataset)*(1-val_share))\n",
    "    print (\"Offest:\" + str(val_offset))\n",
    "    return FullTrainningDataset(dataset, 0, val_offset), FullTrainningDataset(dataset, \n",
    "                                                                              val_offset, len(dataset)-val_offset)\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# train_imgs = torch.from_numpy(full_img_tr).float()\n",
    "train_imgs=XnumpyToTensor (full_img)\n",
    "train_targets = YnumpyToTensor(data['is_iceberg'].values)\n",
    "dset_train = TensorDataset(train_imgs, train_targets)\n",
    "\n",
    "\n",
    "train_ds, val_ds = trainTestSplit(dset_train)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=False,\n",
    "                                            num_workers=1)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "print (train_loader)\n",
    "print (val_loader)\n",
    "\n",
    "num_epoches = 5\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "import attr\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "n_channels = 2  # max 20\n",
    "total_classes = 1\n",
    "    \n",
    "\n",
    "# https://github.com/Lextal/pspnet-pytorch/blob/master/train.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Funct\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# class SegNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SegNet, self).__init__()\n",
    "\n",
    "#         self.encoder_1 = nn.Sequential(\n",
    "#             nn.Conv2d(2, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d((2, 2), stride=(2, 2), return_indices=True)\n",
    "#         )  # first group\n",
    "\n",
    "#         self.encoder_2 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d((2, 2), stride=(2, 2), return_indices=True)\n",
    "#         )  # second group\n",
    "\n",
    "#         self.encoder_3 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d((2, 2), stride=(2, 2), return_indices=True)\n",
    "#         )  # third group\n",
    "\n",
    "#         self.encoder_4 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d((2, 2), stride=(2, 2), return_indices=True)\n",
    "#         )  # fourth group\n",
    "\n",
    "#         self.unpool_1 = nn.MaxUnpool2d(2, stride=2)  # get masks\n",
    "#         self.unpool_2 = nn.MaxUnpool2d(2, stride=2)\n",
    "#         self.unpool_3 = nn.MaxUnpool2d(2, stride=2)\n",
    "#         self.unpool_4 = nn.MaxUnpool2d(2, stride=2)\n",
    "\n",
    "#         self.decoder_1 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64)\n",
    "#         )  # first group\n",
    "\n",
    "#         self.decoder_2 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64)\n",
    "#         )  # second group\n",
    "\n",
    "#         self.decoder_3 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 7, padding=3),\n",
    "#             nn.BatchNorm2d(64)\n",
    "#         )  # third group\n",
    "\n",
    "#         self.decoder_4 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 3, 7, padding=3),\n",
    "#             nn.BatchNorm2d(3)\n",
    "#         )  # fourth group\n",
    "\n",
    "#         # self.conv_classifier = nn.Conv2d(128, 5, 1)\n",
    "        \n",
    "#         self.classifier = torch.nn.Sequential(\n",
    "#             nn.Linear(972, 1),             \n",
    "#         )\n",
    "        \n",
    "#         self.mp = nn.MaxPool2d(4, 4)\n",
    "        \n",
    "#         self.sig = nn.Sigmoid()   \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         size_1 = x.size()\n",
    "#         x, indices_1 = self.encoder_1(x)\n",
    "\n",
    "#         size_2 = x.size()\n",
    "#         x, indices_2 = self.encoder_2(x)\n",
    "\n",
    "#         size_3 = x.size()\n",
    "#         x, indices_3 = self.encoder_3(x)\n",
    "\n",
    "#         size_4 = x.size()\n",
    "#         x, indices_4 = self.encoder_4(x)\n",
    "\n",
    "#         x = self.unpool_1(x, indices_4, output_size=size_4)\n",
    "#         x = self.decoder_1(x)\n",
    "\n",
    "#         x = self.unpool_2(x, indices_3, output_size=size_3)\n",
    "#         x = self.decoder_2(x)\n",
    "\n",
    "#         x = self.unpool_3(x, indices_2, output_size=size_2)\n",
    "#         x = self.decoder_3(x)\n",
    "\n",
    "#         x = self.unpool_4(x, indices_1, output_size=size_1)\n",
    "#         x = self.decoder_4(x)\n",
    "        \n",
    "#         x = self.mp(x)\n",
    "#         x = x.view(x.size(0), -1)    \n",
    "#         print(\"shape:\" + str(x.data.shape))\n",
    "#         x = self.classifier(x)\n",
    "#         # print(\"shape:\" + str(x.data.shape))\n",
    "\n",
    "#         x = self.sig(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "# model=SegNet()\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        interChannels = 4*growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
    "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class SingleLayer(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(SingleLayer, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, nChannels, nOutChannels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        nDenseBlocks = (depth-4) // 3\n",
    "        if bottleneck:\n",
    "            nDenseBlocks //= 2\n",
    "\n",
    "        nChannels = 2*growthRate\n",
    "        self.conv1 = nn.Conv2d(2, nChannels, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans1 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans2 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.fc = nn.Linear(128, nClasses)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
    "        layers = []\n",
    "        for i in range(int(nDenseBlocks)):\n",
    "            if bottleneck:\n",
    "                layers.append(Bottleneck(nChannels, growthRate))\n",
    "            else:\n",
    "                layers.append(SingleLayer(nChannels, growthRate))\n",
    "            nChannels += growthRate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.dense3(out)\n",
    "        # print(out.data.shape)\n",
    "        out = F.avg_pool2d(F.relu(self.bn1(out)), 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # print(out.data.shape)\n",
    "        out = F.sigmoid(self.fc(out))\n",
    "        return out\n",
    "\n",
    "model = DenseNet(growthRate=8, depth=20, reduction=0.5,\n",
    "                            bottleneck=True, nClasses=1)\n",
    "\n",
    "print('  + Number of params: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n",
    "        \n",
    "print(model)\n",
    "# https://github.com/ZijunDeng/pytorch-semantic-segmentation/tree/master/models\n",
    "# https://github.com/andreasveit/densenet-pytorch/blob/master/densenet.py\n",
    "# https://github.com/meliketoy/wide-resnet.pytorch/blob/master/networks/wide_resnet.py\n",
    "\n",
    "# # Loss and optimizer\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "'''DenseNet in PyTorch.'''\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, growth_rate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, 4 * growth_rate, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4 * growth_rate)\n",
    "        self.conv2 = nn.Conv2d(4 * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.mp = torch.nn.MaxPool2d(1, 1)\n",
    "        # self.avgpool = torch.nn.AvgPool2d(2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        # out = self.mp(out)\n",
    "        # out = self.avgpool(out)\n",
    "\n",
    "        # print (x.data.shape)\n",
    "\n",
    "        out = torch.cat([out, x], 1)\n",
    "        out = self.mp(out)\n",
    "        # out = self.avgpool(out)\n",
    "        # print(out.data.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_planes)\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(F.relu(self.bn(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=1):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        num_planes = 2 * growth_rate\n",
    "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
    "        num_planes += nblocks[0] * growth_rate\n",
    "        out_planes = int(math.floor(num_planes * reduction))\n",
    "        self.trans1 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
    "        num_planes += nblocks[1] * growth_rate\n",
    "        out_planes = int(math.floor(num_planes * reduction))\n",
    "        self.trans2 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
    "        num_planes += nblocks[2] * growth_rate\n",
    "        out_planes = int(math.floor(num_planes * reduction))\n",
    "        self.trans3 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
    "        num_planes += nblocks[3] * growth_rate\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(num_planes)\n",
    "\n",
    "        self.linear = nn.Linear(3328, num_classes)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def _make_dense_layers(self, block, in_planes, nblock):\n",
    "        layers = []\n",
    "        for i in range(nblock):\n",
    "            layers.append(block(in_planes, self.growth_rate))\n",
    "            in_planes += self.growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # print (out.data.shape)\n",
    "        out = self.linear(out)\n",
    "        out = self.sig(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def DenseNet121():\n",
    "    return DenseNet(Bottleneck, [6, 12, 24, 16], growth_rate=12)\n",
    "\n",
    "\n",
    "def DenseNet169():\n",
    "    return DenseNet(Bottleneck, [6, 12, 32, 32], growth_rate=16)\n",
    "\n",
    "\n",
    "def DenseNet201():\n",
    "    return DenseNet(Bottleneck, [6, 12, 48, 32], growth_rate=32)\n",
    "\n",
    "\n",
    "def DenseNet161():\n",
    "    return DenseNet(Bottleneck, [6, 12, 36, 24], growth_rate=48)\n",
    "\n",
    "\n",
    "def densenet_cifar():\n",
    "    return DenseNet(Bottleneck, [6, 12, 24, 16], growth_rate=12)\n",
    "\n",
    "# test_densenet()\n",
    "\n",
    "\n",
    "\n",
    "loss_func=torch.nn.BCELoss() # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "\n",
    "# NN params\n",
    "LR = 0.0005\n",
    "MOMENTUM= 0.95\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=5e-5) #  L2 regularization\n",
    "if use_cuda:\n",
    "    lgr.info (\"Using the GPU\")    \n",
    "    model.cuda()\n",
    "    loss_func.cuda()\n",
    "\n",
    "lgr.info (optimizer)\n",
    "lgr.info (loss_func)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "criterion = loss_func\n",
    "all_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    for epoch in range(num_epoches):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epoches))\n",
    "        print('*' * 5 + ':')\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        for i, data in enumerate(train_loader, 1):\n",
    "    \n",
    "            img, label = data\n",
    "            if use_cuda:\n",
    "                img, label = Variable(img.cuda(async=True)), Variable(label.cuda(async=True))  # On GPU\n",
    "            else:\n",
    "                img, label = Variable(img), Variable(\n",
    "                    label)  # RuntimeError: expected CPU tensor (got CUDA tensor)\n",
    "    \n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            running_loss += loss.data[0] * label.size(0)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            if i % 10 == 0:\n",
    "                all_losses.append(running_loss / (batch_size * i))\n",
    "                print('[{}/{}] Loss: {:.6f}'.format(\n",
    "                    epoch + 1, num_epoches, running_loss / (batch_size * i),\n",
    "                    running_acc / (batch_size * i)))\n",
    "    \n",
    "        print('Finish {} epoch, Loss: {:.6f}'.format(epoch + 1, running_loss / (len(train_ds))))\n",
    "    \n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "        for data in val_loader:\n",
    "            img, label = data\n",
    "    \n",
    "            if use_cuda:\n",
    "                img, label = Variable(img.cuda(async=True), volatile=True),\n",
    "                Variable(label.cuda(async=True), volatile=True)  # On GPU\n",
    "            else:\n",
    "                img = Variable(img, volatile=True)\n",
    "                label = Variable(label, volatile=True)\n",
    "    \n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            eval_loss += loss.data[0] * label.size(0)\n",
    "    \n",
    "        print('VALIDATION Loss: {:.6f}'.format(eval_loss / (len(val_ds))))\n",
    "        val_losses.append(eval_loss / (len(val_ds)))\n",
    "        print()\n",
    "    \n",
    "    torch.save(model.state_dict(), './cnn.pth')\n",
    "    \n",
    "    \n",
    "    df_test_set = pd.read_json('../input/test.json')\n",
    "    \n",
    "    df_test_set['band_1'] = df_test_set['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "    df_test_set['band_2'] = df_test_set['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "    df_test_set['inc_angle'] = pd.to_numeric(df_test_set['inc_angle'], errors='coerce')\n",
    "    \n",
    "    df_test_set.head(3)\n",
    "    \n",
    "    \n",
    "    print (df_test_set.shape)\n",
    "    columns = ['id', 'is_iceberg']\n",
    "    df_pred=pd.DataFrame(data=np.zeros((0,len(columns))), columns=columns)\n",
    "    # df_pred.id.astype(int)\n",
    "    \n",
    "    for index, row in df_test_set.iterrows():\n",
    "        rwo_no_id=row.drop('id')    \n",
    "        band_1_test = (rwo_no_id['band_1']).reshape(-1, 75, 75)\n",
    "        band_2_test = (rwo_no_id['band_2']).reshape(-1, 75, 75)\n",
    "        full_img_test = np.stack([band_1_test, band_2_test], axis=1)\n",
    "    \n",
    "        x_data_np = np.array(full_img_test, dtype=np.float32)        \n",
    "        if use_cuda:\n",
    "            X_tensor_test = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "        else:\n",
    "            X_tensor_test = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "                        \n",
    "    #     X_tensor_test=X_tensor_test.view(1, trainX.shape[1]) # does not work with 1d tensors            \n",
    "        predicted_val = (model(X_tensor_test).data).float() # probabilities     \n",
    "        p_test =   predicted_val.cpu().numpy().item() # otherwise we get an array, we need a single float\n",
    "        \n",
    "        df_pred = df_pred.append({'id':row['id'], 'is_iceberg':p_test},ignore_index=True)\n",
    "    #     df_pred = df_pred.append({'id':row['id'].astype(int), 'probability':p_test},ignore_index=True)\n",
    "    \n",
    "    df_pred.head(5)\n",
    "    \n",
    "    \n",
    "    def savePred(df_pred):\n",
    "    #     csv_path = 'pred/p_{}_{}_{}.csv'.format(loss, name, (str(time.time())))\n",
    "    #     csv_path = 'pred_{}_{}.csv'.format(loss, (str(time.time())))\n",
    "        csv_path='denseNet201801131316.csv'\n",
    "        df_pred.to_csv(csv_path, columns=('id', 'is_iceberg'), index=None)\n",
    "        print (csv_path)\n",
    "        \n",
    "    savePred (df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
