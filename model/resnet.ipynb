{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "images (InputLayer)             (None, 75, 75, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 75, 75, 3)    12          images[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 75, 75, 32)   896         batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 37, 37, 32)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 37, 37, 32)   0           max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 37, 37, 32)   128         dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 37, 37, 64)   18496       batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 18, 18, 64)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 18, 18, 64)   0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 18, 18, 64)   256         dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 18, 18, 128)  73856       batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 18, 18, 128)  0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 18, 18, 128)  512         dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 18, 18, 64)   73792       batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 18, 18, 64)   0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 18, 18, 64)   0           dropout_16[0][0]                 \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 18, 18, 64)   256         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 128)  73856       batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 8, 8, 128)    0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 8, 8, 128)    0           max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "angle (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_3 (GlobalM (None, 128)          0           dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 1)            4           angle[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 129)          0           global_max_pooling2d_3[0][0]     \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          33280       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 256)          1024        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 256)          0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           16448       dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 64)           256         dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 64)           0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            65          dropout_21[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 293,137\n",
      "Trainable params: 291,913\n",
      "Non-trainable params: 1,224\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      " - 12s - loss: 0.6128 - acc: 0.7213 - val_loss: 0.3321 - val_acc: 0.8688\n",
      "Epoch 2/20\n",
      " - 10s - loss: 0.4689 - acc: 0.7966 - val_loss: 0.3678 - val_acc: 0.7934\n",
      "Epoch 3/20\n",
      " - 9s - loss: 0.3950 - acc: 0.8307 - val_loss: 0.3965 - val_acc: 0.7902\n",
      "Epoch 4/20\n",
      " - 9s - loss: 0.3964 - acc: 0.8282 - val_loss: 0.3288 - val_acc: 0.8656\n",
      "Epoch 5/20\n",
      " - 10s - loss: 0.3640 - acc: 0.8390 - val_loss: 0.3443 - val_acc: 0.8492\n",
      "Epoch 6/20\n",
      " - 10s - loss: 0.3485 - acc: 0.8412 - val_loss: 0.3112 - val_acc: 0.8623\n",
      "Epoch 7/20\n",
      " - 9s - loss: 0.3581 - acc: 0.8498 - val_loss: 0.3567 - val_acc: 0.8295\n",
      "Epoch 8/20\n",
      " - 9s - loss: 0.3225 - acc: 0.8653 - val_loss: 0.2842 - val_acc: 0.9016\n",
      "Epoch 9/20\n",
      " - 9s - loss: 0.3400 - acc: 0.8476 - val_loss: 0.2847 - val_acc: 0.8787\n",
      "Epoch 10/20\n",
      " - 9s - loss: 0.3180 - acc: 0.8678 - val_loss: 0.3119 - val_acc: 0.8656\n",
      "Epoch 11/20\n",
      " - 9s - loss: 0.3429 - acc: 0.8554 - val_loss: 0.3257 - val_acc: 0.8689\n",
      "Epoch 12/20\n",
      " - 9s - loss: 0.3066 - acc: 0.8662 - val_loss: 0.2686 - val_acc: 0.8787\n",
      "Epoch 13/20\n",
      " - 10s - loss: 0.3161 - acc: 0.8724 - val_loss: 0.2694 - val_acc: 0.8721\n",
      "Epoch 14/20\n",
      " - 9s - loss: 0.3190 - acc: 0.8625 - val_loss: 0.2837 - val_acc: 0.8820\n",
      "Epoch 15/20\n",
      " - 9s - loss: 0.2923 - acc: 0.8794 - val_loss: 0.2758 - val_acc: 0.8918\n",
      "Epoch 16/20\n",
      " - 9s - loss: 0.3127 - acc: 0.8695 - val_loss: 0.2955 - val_acc: 0.8787\n",
      "Epoch 17/20\n",
      " - 9s - loss: 0.3138 - acc: 0.8683 - val_loss: 0.3105 - val_acc: 0.8590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n",
      " - 9s - loss: 0.3094 - acc: 0.8657 - val_loss: 0.2599 - val_acc: 0.8852\n",
      "Epoch 19/20\n",
      " - 9s - loss: 0.2886 - acc: 0.8857 - val_loss: 0.2613 - val_acc: 0.8852\n",
      "Epoch 20/20\n",
      " - 10s - loss: 0.2708 - acc: 0.8873 - val_loss: 0.2658 - val_acc: 0.8820\n",
      "Model fitting done. Total time: 3m 14s\n",
      "321/321 [==============================] - 0s 284us/step\n",
      "Validation score: 0.24387\n",
      "Validation accuracy: 89.72%\n",
      "('====================', '\\n')\n",
      "Loading and evaluating on test data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, Dense, Dropout, BatchNormalization, Input, Flatten, Activation\n",
    "from keras.layers.merge import Concatenate, add\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "\n",
    "def build_model( baseline_cnn = False ):\n",
    "    #Based on kernel https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d\n",
    "    image_input = Input( shape = (75, 75, 3), name = 'images' )\n",
    "    angle_input = Input( shape = [1], name = 'angle' )\n",
    "    activation = 'elu'\n",
    "    bn_momentum = 0.99\n",
    "    \n",
    "    # Simple CNN as baseline model\n",
    "    if baseline_cnn:\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add( Conv2D(16, kernel_size = (3, 3), activation = 'relu', input_shape = (75, 75, 3)) )\n",
    "        model.add( BatchNormalization(momentum = bn_momentum) )\n",
    "        model.add( MaxPooling2D(pool_size = (3, 3), strides = (2, 2)) )\n",
    "        model.add( Dropout(0.2) )\n",
    "\n",
    "        model.add( Conv2D(32, kernel_size = (3, 3), activation = 'relu') )\n",
    "        model.add( BatchNormalization(momentum = bn_momentum) )\n",
    "        model.add( MaxPooling2D(pool_size = (2, 2), strides = (2, 2)) )\n",
    "        model.add( Dropout(0.2) )\n",
    "\n",
    "        model.add( Conv2D(64, kernel_size = (3, 3), activation = 'relu') )\n",
    "        model.add( BatchNormalization(momentum = bn_momentum) )\n",
    "        model.add( MaxPooling2D(pool_size = (2, 2), strides = (2, 2)) )\n",
    "        model.add( Dropout(0.2) )\n",
    "\n",
    "        model.add( Conv2D(128, kernel_size = (3, 3), activation = 'relu') )\n",
    "        model.add( BatchNormalization(momentum = bn_momentum) )\n",
    "        model.add( MaxPooling2D(pool_size = (2, 2), strides = (2, 2)) )\n",
    "        model.add( Dropout(0.2) )\n",
    "\n",
    "        model.add( Flatten() )\n",
    "\n",
    "        model.add( Dense(256, activation = 'relu') )\n",
    "        model.add( BatchNormalization(momentum = bn_momentum) )\n",
    "        model.add( Dropout(0.3) )\n",
    "\n",
    "        model.add( Dense(128, activation = 'relu') )\n",
    "        model.add( BatchNormalization(momentum = bn_momentum) )\n",
    "        model.add( Dropout(0.3) )\n",
    "\n",
    "        model.add( Dense(1, activation = 'sigmoid') )\n",
    "\n",
    "        opt = Adam( lr = 1e-3, beta_1 = .9, beta_2 = .999, decay = 1e-3 )\n",
    "\n",
    "        model.compile( loss = 'binary_crossentropy', optimizer = opt, metrics = ['accuracy'] )\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "    else:\n",
    "        img_1 = Conv2D( 32, kernel_size = (3, 3), activation = activation, padding = 'same' ) ((BatchNormalization(momentum=bn_momentum) ) ( image_input) )\n",
    "        img_1 = MaxPooling2D( (2,2)) (img_1 )\n",
    "        img_1 = Dropout( 0.2 )( img_1 )\n",
    "\n",
    "        img_1 = Conv2D( 64, kernel_size = (3, 3), activation = activation, padding = 'same' ) ( (BatchNormalization(momentum=bn_momentum)) (img_1) )\n",
    "        img_1 = MaxPooling2D( (2,2) ) ( img_1 )\n",
    "        img_1 = Dropout( 0.2 )( img_1 )\n",
    "  \n",
    "         # Residual block\n",
    "        img_2 = Conv2D( 128, kernel_size = (3, 3), activation = activation, padding = 'same' ) ( (BatchNormalization(momentum=bn_momentum)) (img_1) )\n",
    "        img_2 = Dropout(0.2) ( img_2 )\n",
    "        img_2 = Conv2D( 64, kernel_size = (3, 3), activation = activation, padding = 'same' ) ( (BatchNormalization(momentum=bn_momentum)) (img_2) )\n",
    "        img_2 = Dropout(0.2) ( img_2 )\n",
    "        \n",
    "        img_res = add( [img_1, img_2] )\n",
    "\n",
    "        # Filter resudial output\n",
    "        img_res = Conv2D( 128, kernel_size = (3, 3), activation = activation ) ( (BatchNormalization(momentum=bn_momentum)) (img_res) )\n",
    "        img_res = MaxPooling2D( (2,2) ) ( img_res )\n",
    "        img_res = Dropout( 0.2 )( img_res )\n",
    "        img_res = GlobalMaxPooling2D() ( img_res )\n",
    "        \n",
    "        cnn_out = ( Concatenate()( [img_res, BatchNormalization(momentum=bn_momentum)(angle_input)]) )\n",
    "\n",
    "        dense_layer = Dropout( 0.5 ) ( BatchNormalization(momentum=bn_momentum) (Dense(256, activation = activation) (cnn_out)) )\n",
    "        dense_layer = Dropout( 0.5 ) ( BatchNormalization(momentum=bn_momentum) (Dense(64, activation = activation) (dense_layer)) )\n",
    "        output = Dense( 1, activation = 'sigmoid' ) ( dense_layer )\n",
    "        \n",
    "        model = Model( [image_input, angle_input], output )\n",
    "\n",
    "        opt = Adam( lr = 1e-3, beta_1 = .9, beta_2 = .999, decay = 1e-3 )\n",
    "\n",
    "        model.compile( loss = 'binary_crossentropy', optimizer = opt, metrics = ['accuracy'] )\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_callbacks( weight_save_path, no_improv_epochs = 10, min_delta = 1e-4 ):\n",
    "    es = EarlyStopping( 'val_loss', patience = no_improv_epochs, mode = 'min', min_delta = min_delta )\n",
    "    ms = ModelCheckpoint( weight_save_path, 'val_loss', save_best_only = True )\n",
    "\n",
    "    return [ es, ms ]\n",
    "\n",
    "def generate_data( data ):\n",
    "    X_band_1=np.array( [np.array(band).astype(np.float32).reshape(75, 75) \n",
    "                        for band in data['band_1']] )\n",
    "    X_band_2=np.array( [np.array(band).astype(np.float32).reshape(75, 75) \n",
    "                        for band in data['band_2']] )\n",
    "    X = np.concatenate( [X_band_1[:, :, :, np.newaxis], X_band_2[:, :, :, np.newaxis], \\\n",
    "                        ((X_band_1 + X_band_2)/2)[:, :, :, np.newaxis]], axis=-1 )\n",
    "    return X\n",
    "\n",
    "def augment_data( generator, X1, X2, y, batch_size = 32 ):\n",
    "    generator_seed = np.random.randint( 9999 )\n",
    "    gen_X1 = generator.flow( X1, y, batch_size = batch_size, seed = generator_seed )\n",
    "    gen_X2 = generator.flow( X1, X2, batch_size = batch_size, seed = generator_seed )\n",
    "\n",
    "    while True:\n",
    "        X1i = gen_X1.next()\n",
    "        X2i = gen_X2.next()\n",
    "\n",
    "        yield [ X1i[0], X2i[1] ], X1i[1]\n",
    "    \n",
    "def plot_band_samples( data, band = 1, title = None ):\n",
    "    fig = plt.figure( 1, figsize=(15, 15) )\n",
    "    for i in range(9):\n",
    "        ax = fig.add_subplot( 3, 3, i + 1 )\n",
    "        arr = np.reshape( np.array(data.iloc[i, band - 1]), (75, 75) )\n",
    "        ax.imshow( arr, cmap='inferno' )\n",
    "        fig.suptitle( title )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_all_bands( data, title = None ):\n",
    "    fig = plt.figure( 1, figsize = (15, 15) )\n",
    "    count = 1\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            ax = fig.add_subplot( 3, 3, count )\n",
    "            ax.imshow( data[i, :, :, j], cmap = 'inferno' )\n",
    "            count += 1\n",
    "            if i == 0:\n",
    "                if j == 0:\n",
    "                    ax.set_title( 'Band 1' , fontsize = 12)\n",
    "                elif j == 1:\n",
    "                    ax.set_title( 'Band 2', fontsize = 12 )\n",
    "                elif j == 2:\n",
    "                    ax.set_title( 'Average', fontsize = 12 )\n",
    "    fig.suptitle( title, fontsize = 14, fontweight = 'bold' )\n",
    "    plt.show()\n",
    "\n",
    "def make_plots( data, band_samples = True, all_bands = True ):\n",
    "    ships = data[ data.is_iceberg == 0 ].sample( n = 9, random_state = 42 )\n",
    "    icebergs = data[ data.is_iceberg == 1 ].sample( n = 9, random_state = 42 )\n",
    "\n",
    "    np_ships = generate_data( ships )\n",
    "    np_icebergs = generate_data( icebergs )\n",
    "\n",
    "    if band_samples:\n",
    "        plot_band_samples( ships, band = 2, title = 'Ship image samples' )\n",
    "        plot_band_samples( icebergs, band = 2, title = 'Iceberg image samples' )\n",
    "\n",
    "    if all_bands:\n",
    "        plot_all_bands( np_ships, 'Image bands for ships' )\n",
    "        plot_all_bands( np_icebergs, 'Image bands for icebergs' )\n",
    "\n",
    " \n",
    " \n",
    " \n",
    "TEST = True # Should test data be passed to the model?\n",
    "DO_PLOT = False # Exploratory data plots\n",
    "USE_AUGMENTATION = True # Whether or not image augmentations should be made\n",
    "TRAIN_PATH = 'kaggle_lceberg_data/train.json'\n",
    "TEST_PATH = 'kaggle_lceberg_data/test.json'\n",
    "WEIGHT_SAVE_PATH = 'model_weights.hdf5'\n",
    "PREDICTION_SAVE_PATH = 'kaggle_lceberg_data/test_submission.csv'\n",
    "\n",
    "if TEST:\n",
    "    SEED = np.random.randint( 9999 )\n",
    "else:\n",
    "    SEED = 42 # Constant seed for comparability between runs\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20 # Increase this\n",
    "\n",
    "train_data = pd.read_json( TRAIN_PATH )\n",
    "train_data[ 'inc_angle' ] = train_data[ 'inc_angle' ].replace('na', 0)\n",
    "train_data[ 'inc_angle' ] = train_data[ 'inc_angle' ].astype(float).fillna(0.0)\n",
    "\n",
    "X = generate_data( train_data )\n",
    "X_a = train_data[ 'inc_angle' ]\n",
    "y = train_data[ 'is_iceberg' ]\n",
    "\n",
    "if DO_PLOT:\n",
    "    make_plots( train_data, band_samples = True, all_bands = True )\n",
    "\n",
    "X_train, X_val, X_angle_train, X_angle_val, y_train, y_val = train_test_split( X, X_a, y, train_size = .8, random_state = SEED )\n",
    "callback_list = get_callbacks( WEIGHT_SAVE_PATH, 20 )\n",
    "\n",
    "model = build_model()\n",
    "start_time = time.time()\n",
    "\n",
    "if USE_AUGMENTATION:\n",
    "    image_augmentation = ImageDataGenerator( rotation_range = 20,\n",
    "                                             horizontal_flip = True,\n",
    "                                             vertical_flip = True,\n",
    "                                             width_shift_range = .3,\n",
    "                                             height_shift_range =.3,\n",
    "                                             zoom_range = .1 )\n",
    "    input_generator = augment_data( image_augmentation, X_train, X_angle_train, y_train, batch_size = BATCH_SIZE )\n",
    "\n",
    "    model.fit_generator( input_generator, steps_per_epoch = 4096/BATCH_SIZE, epochs = EPOCHS,\n",
    "                         callbacks = callback_list, verbose = 2, \n",
    "                         validation_data = augment_data(image_augmentation, X_val, X_angle_val, y_val, batch_size = BATCH_SIZE),\n",
    "                         validation_steps = len(X_val)/BATCH_SIZE )\n",
    "\n",
    "else: \n",
    "    # Just fit model to the given training data\n",
    "    model.fit( [X_train, X_angle_train], y_train, batch_size = BATCH_SIZE, epochs = EPOCHS, verbose = 2, \n",
    "               validation_data = ([X_val, X_angle_val], y_val), callbacks = callback_list )\n",
    "\n",
    "m, s = divmod( time.time() - start_time, 60 )\n",
    "print( 'Model fitting done. Total time: {}m {}s'.format(int(m), int(s)) )\n",
    "\n",
    "model.load_weights( WEIGHT_SAVE_PATH )\n",
    "val_score = model.evaluate( [X_val, X_angle_val], y_val, verbose = 1 )\n",
    "print( 'Validation score: {}'.format(round(val_score[0], 5)) )\n",
    "print( 'Validation accuracy: {}%'.format(round(val_score[1]*100, 2)) )\n",
    "print( '='*20, '\\n' )\n",
    "\n",
    "if TEST:\n",
    "    print( 'Loading and evaluating on test data' )\n",
    "    test_data = pd.read_json( TEST_PATH )\n",
    "\n",
    "    X_test = generate_data( test_data )\n",
    "    X_a_test = test_data[ 'inc_angle' ]\n",
    "    test_predictions = model.predict( [X_test, X_a_test] )\n",
    "\n",
    "    submission = pd.DataFrame()\n",
    "    submission[ 'id' ] = test_data[ 'id' ]\n",
    "    submission[ 'is_iceberg' ] = test_predictions.reshape( (test_predictions.shape[0]) )\n",
    "\n",
    "    submission.to_csv( PREDICTION_SAVE_PATH, index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
